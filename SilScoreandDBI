# -*- coding: utf-8 -*-
"""ClusteringWithSilhoutteScoreandDBI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12kVAZLtAQ7n4YxWO8z2E4Hmr7Wh-YCiB
"""

!pip install scikit-learn scikit-learn-extra

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score
import time

#the datasets
datasets = {
    "Banknote": pd.read_csv('banknote.csv'),
    "Ionosphere": pd.read_csv('ionosphere.csv'),
    "Sonar": pd.read_csv('sonar.csv'),
    "Blobs": pd.read_csv('blobs.csv'),
    "Varied": pd.read_csv('varied.csv'),
    "Wine": pd.read_csv('wine.csv'),
    "Flame": pd.read_csv('flame.csv'),
    "Glass": pd.read_csv('glass.csv'),
    "Iris": pd.read_csv('iris.csv'),

}

#function to perform clustering kmeans kmedoids gmm
def perform_clustering(data, n_clusters):
    results = {}

    # K-Means
    start_time = time.time()
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(data)
    results['KMeans'] = {
        "labels": kmeans.labels_,
        "time": time.time() - start_time,
        "silhouette": silhouette_score(data, kmeans.labels_),
        "davies_bouldin": davies_bouldin_score(data, kmeans.labels_)
    }

    # K-Medoids
    start_time = time.time()
    kmedoids = KMedoids(n_clusters=n_clusters, random_state=42).fit(data)
    results['KMedoids'] = {
        "labels": kmedoids.labels_,
        "time": time.time() - start_time,
        "silhouette": silhouette_score(data, kmedoids.labels_),
        "davies_bouldin": davies_bouldin_score(data, kmedoids.labels_)
    }

    # GMM
    start_time = time.time()
    gmm = GaussianMixture(n_components=n_clusters, random_state=42).fit(data)
    gmm_labels = gmm.predict(data)
    results['GMM'] = {
        "labels": gmm_labels,
        "time": time.time() - start_time,
        "silhouette": silhouette_score(data, gmm_labels),
        "davies_bouldin": davies_bouldin_score(data, gmm_labels)
    }

    return results

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Function to plot dataset distribution in raw form
def plot_dataset_distribution(data, title):
    plt.scatter(data[:, 0], data[:, 1], s=30, alpha=0.7)
    plt.title(title)
    plt.show()

# Loop through each dataset
for name, dataset in datasets.items():
    print(f"Visualizing {name} dataset before clustering...")

    # Preprocess dataset (PCA to reduce to 2D for visualization)
    data = PCA(n_components=2).fit_transform(dataset.iloc[:, :-1])  # Exclude target column

    # Plot the dataset distribution
    plot_dataset_distribution(data, f"{name} Dataset Distribution (Before Clustering)")

#General Visualization Function
import matplotlib.pyplot as plt

def plot_clusters(data, labels, title):
    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=30)
    plt.title(title)
    plt.show()

from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Function to plot clusters
def plot_clusters(data, labels, title):
    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=30)
    plt.title(title)
    plt.show()

# Define number of clusters for each dataset
cluster_counts = {
    "Banknote": 2,
    "Ionosphere": 2,
    "Sonar": 2,
    "Blobs": 3,
    "Varied": 3,
    "Wine": 3,
    "Flame": 2,
    "Glass": 6,
    "Iris": 3
}

# Clustering methods to visualize
clustering_methods = {
    "K-Means": KMeans,
    "K-Medoids": KMedoids,
    "GMM": GaussianMixture
}

# Loop through each dataset and clustering method
for method_name, method in clustering_methods.items():
    for name, dataset in datasets.items():
        print(f"Processing {name} with {method_name}...")

        # Preprocess dataset (PCA to reduce to 2D for visualization)
        data = PCA(n_components=2).fit_transform(dataset.iloc[:, :-1])  # Exclude target column

        # Get the number of clusters for the dataset
        n_clusters = cluster_counts.get(name, 2)  # Default to 2 if not specified

        # Fit the clustering model
        if method_name == "GMM":
            model = method(n_components=n_clusters, random_state=42)
        else:
            model = method(n_clusters=n_clusters, random_state=42)
        labels = model.fit_predict(data)

        # Visualize the results
        plot_clusters(data, labels, f"{name} - {method_name} Clustering ({n_clusters} Clusters)")

#calculatin metrics
import time
from sklearn.metrics import silhouette_score, davies_bouldin_score
import pandas as pd
from sklearn.decomposition import PCA

# Initialize results dictionary
results = {}

# Loop through each dataset and clustering method
for method_name, method in clustering_methods.items():
    for name, dataset in datasets.items():
        print(f"Processing {name} with {method_name}...")

        # Preprocess dataset (PCA to reduce to 2D for visualization)
        data = PCA(n_components=2).fit_transform(dataset.iloc[:, :-1])  # Exclude target column

        # Get the number of clusters for the dataset
        n_clusters = cluster_counts.get(name, 2)  # Default to 2 if not specified

        # Track execution time
        start_time = time.time()

        # Fit the clustering model
        if method_name == "GMM":
            model = method(n_components=n_clusters, random_state=42)
        else:
            model = method(n_clusters=n_clusters, random_state=42)

        labels = model.fit_predict(data)
        execution_time = time.time() - start_time

        # Calculate metrics
        silhouette = silhouette_score(data, labels)
        davies_bouldin = davies_bouldin_score(data, labels)

        # Store results
        results[f"{name}-{method_name}"] = {
            "Silhouette Score": silhouette,
            "Davies-Bouldin Index": davies_bouldin,
            "Execution Time (s)": execution_time,
        }

# Convert results to a DataFrame
results_df = pd.DataFrame(results).T
results_df.index.name = "Dataset-Method"

# Display metrics in the notebook
print(results_df)

# Optional: Save results to a CSV file for further analysis
results_df.to_csv("clustering_metrics.csv", index=True)
print("Results saved to 'clustering_metrics.csv'")







