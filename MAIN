# -*- coding: utf-8 -*-
"""ClusteringUpdatedCode-MainDocument.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kMl9UQ1ilZr5s9waw0EsIBZKFdpIz3w-
"""

!pip install scikit-learn scikit-learn-extra

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score
import time

#the datasets
datasets = {
    "Banknote": pd.read_csv('banknote.csv'),
    "Ionosphere": pd.read_csv('ionosphere.csv'),
    "Sonar": pd.read_csv('sonar.csv'),
    "Blobs": pd.read_csv('blobs.csv'),
    "Varied": pd.read_csv('varied.csv'),
    "Wine": pd.read_csv('wine.csv'),
    "Flame": pd.read_csv('flame.csv'),
    "Glass": pd.read_csv('glass.csv'),
    "Iris": pd.read_csv('iris.csv'),

}

#function to perform clustering kmeans kmedoids gmm
def perform_clustering(data, n_clusters):
    results = {}

    # K-Means
    start_time = time.time()
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(data)
    results['KMeans'] = {
        "labels": kmeans.labels_,
        "time": time.time() - start_time,
        "silhouette": silhouette_score(data, kmeans.labels_),
        "davies_bouldin": davies_bouldin_score(data, kmeans.labels_)
    }

    # K-Medoids
    start_time = time.time()
    kmedoids = KMedoids(n_clusters=n_clusters, random_state=42).fit(data)
    results['KMedoids'] = {
        "labels": kmedoids.labels_,
        "time": time.time() - start_time,
        "silhouette": silhouette_score(data, kmedoids.labels_),
        "davies_bouldin": davies_bouldin_score(data, kmedoids.labels_)
    }

    # GMM
    start_time = time.time()
    gmm = GaussianMixture(n_components=n_clusters, random_state=42).fit(data)
    gmm_labels = gmm.predict(data)
    results['GMM'] = {
        "labels": gmm_labels,
        "time": time.time() - start_time,
        "silhouette": silhouette_score(data, gmm_labels),
        "davies_bouldin": davies_bouldin_score(data, gmm_labels)
    }

    return results

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Function to plot dataset distribution in raw form
def plot_dataset_distribution(data, title):
    plt.scatter(data[:, 0], data[:, 1], s=30, alpha=0.7)
    plt.title(title)
    plt.show()

# Loop through each dataset
for name, dataset in datasets.items():
    print(f"Visualizing {name} dataset before clustering...")

    # Preprocess dataset (PCA to reduce to 2D for visualization)
    data = PCA(n_components=2).fit_transform(dataset.iloc[:, :-1])  # Exclude target column

    # Plot the dataset distribution
    plot_dataset_distribution(data, f"{name} Dataset Distribution (Before Clustering)")

#General Visualization Function
import matplotlib.pyplot as plt

def plot_clusters(data, labels, title):
    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=30)
    plt.title(title)
    plt.show()

#clustering step
from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Function to plot clusters
def plot_clusters(data, labels, title):
    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=30)
    plt.title(title)
    plt.show()

# Define number of clusters for each dataset
cluster_counts = {
    "Banknote": 2,
    "Ionosphere": 2,
    "Sonar": 2,
    "Blobs": 3,
    "Varied": 3,
    "Wine": 3,
    "Flame": 2,
    "Glass": 6,
    "Iris": 3
}

# Clustering methods to visualize
clustering_methods = {
    "K-Means": KMeans,
    "K-Medoids": KMedoids,
    "GMM": GaussianMixture
}

# Loop through each dataset and clustering method
for method_name, method in clustering_methods.items():
    for name, dataset in datasets.items():
        print(f"Processing {name} with {method_name}...")

        # Preprocess dataset (PCA to reduce to 2D for visualization)
        data = PCA(n_components=2).fit_transform(dataset.iloc[:, :-1])  # Exclude target column

        # Get the number of clusters for the dataset
        n_clusters = cluster_counts.get(name, 2)  # Default to 2 if not specified

        # Fit the clustering model
        if method_name == "GMM":
            model = method(n_components=n_clusters, random_state=42)
        else:
            model = method(n_clusters=n_clusters, random_state=42)
        labels = model.fit_predict(data)

        # Visualize the results
        plot_clusters(data, labels, f"{name} - {method_name} Clustering ({n_clusters} Clusters)")









!pip install scikit-learn-extra

import time
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn_extra.cluster import KMedoids
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Clustering methods
clustering_methods = {
    "K-Means": KMeans,
    "K-Medoids": KMedoids,
    "GMM": GaussianMixture
}

# Datasets
datasets = {
    "Banknote": pd.read_csv('banknote.csv'),
    "Ionosphere": pd.read_csv('ionosphere.csv'),
    "Sonar": pd.read_csv('sonar.csv'),
    "Blobs": pd.read_csv('blobs.csv'),
    "Varied": pd.read_csv('varied.csv'),
    "Wine": pd.read_csv('wine.csv'),
    "Flame": pd.read_csv('flame.csv'),
    "Glass": pd.read_csv('glass.csv'),
    "Iris": pd.read_csv('iris.csv'),
}

# Define cluster counts for each dataset
cluster_counts = {
    "Banknote": 2,
    "Ionosphere": 2,
    "Sonar": 2,
    "Blobs": 3,
    "Varied": 3,
    "Wine": 3,
    "Flame": 2,
    "Glass": 6,
    "Iris": 3
}

# Function to calculate purity
def calculate_purity(labels, ground_truth):
    contingency_matrix = pd.crosstab(ground_truth, labels)
    purity = np.sum(np.amax(contingency_matrix.values, axis=0)) / np.sum(contingency_matrix.values)
    return purity

# Function to calculate metrics
def calculate_metrics(labels, ground_truth):
    # Purity
    purity = calculate_purity(labels, ground_truth)
    # Accuracy
    accuracy = accuracy_score(ground_truth, labels)
    # Precision, Recall, F1-Score
    precision = precision_score(ground_truth, labels, average="macro", zero_division=0)
    recall = recall_score(ground_truth, labels, average="macro", zero_division=0)
    f1 = f1_score(ground_truth, labels, average="macro", zero_division=0)
    return purity, accuracy, precision, recall, f1

# Initialize results dictionary
results = {}

# Loop through methods and datasets
for method_name, method in clustering_methods.items():
    for dataset_name, dataset in datasets.items():
        print(f"Processing {dataset_name} with {method_name}...")

        # Extract features and ground truth
        data = dataset.iloc[:, :-1].values  # Exclude ground truth column
        ground_truth = dataset.iloc[:, -1].values  # Assume last column contains ground truth

        # Scale data using StandardScaler
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data)

        # Apply PCA for noise reduction (retain 95% variance)
        pca = PCA(n_components=0.95)
        reduced_data = pca.fit_transform(scaled_data)

        # Apply clustering
        if method_name == "GMM":
           model = method(n_components=cluster_counts[dataset_name], random_state=42)
        else:
            model = method(n_clusters=cluster_counts[dataset_name], random_state=42)

        start_time = time.time()
        labels = model.fit_predict(reduced_data)
        execution_time = time.time() - start_time

        # Calculate metrics
        purity, accuracy, precision, recall, f1 = calculate_metrics(labels, ground_truth)

        # Add to results
        results[f"{dataset_name}-{method_name}"] = {
            "Purity": purity,
            "Accuracy": accuracy,
            "Precision": precision,
            "Recall": recall,
            "F1-Score": f1,
            "Execution Time (s)": execution_time
        }

# Convert results to DataFrame
results_df = pd.DataFrame(results).T
results_df.index.name = "Dataset-Method"
print(results_df)

# Save results to CSV
results_df.to_csv("clustering_metrics_kmeans_kmedoids_gmm.csv")
print("Results saved to 'clustering_metrics_kmeans_kmedoids_gmm.csv'")

#updating code to ensure that  clustering results are evaluated in a way that accounts for misalignments between clusters and true labels, leading to more accurate evaluation metrics.
import time
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn_extra.cluster import KMedoids
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.optimize import linear_sum_assignment

# Clustering methods
clustering_methods = {
    "K-Means": KMeans,
    "K-Medoids": KMedoids,
    "GMM": GaussianMixture
}

# Datasets
datasets = {
    "Banknote": pd.read_csv('banknote.csv'),
    "Ionosphere": pd.read_csv('ionosphere.csv'),
    "Sonar": pd.read_csv('sonar.csv'),
    "Blobs": pd.read_csv('blobs.csv'),
    "Varied": pd.read_csv('varied.csv'),
    "Wine": pd.read_csv('wine.csv'),
    "Flame": pd.read_csv('flame.csv'),
    "Glass": pd.read_csv('glass.csv'),
    "Iris": pd.read_csv('iris.csv'),
}

# cluster counts for each dataset
cluster_counts = {
    "Banknote": 2,
    "Ionosphere": 2,
    "Sonar": 2,
    "Blobs": 3,
    "Varied": 3,
    "Wine": 3,
    "Flame": 2,
    "Glass": 6,
    "Iris": 3
}

# Function to calculate purity
def calculate_purity(labels, ground_truth):
    contingency_matrix = pd.crosstab(ground_truth, labels)
    purity = np.sum(np.amax(contingency_matrix.values, axis=0)) / np.sum(contingency_matrix.values)
    return purity

# Function to calculate metrics
def calculate_metrics(labels, ground_truth):
    # Purity
    purity = calculate_purity(labels, ground_truth)
    # Accuracy
    accuracy = accuracy_score(ground_truth, labels)
    # Precision, Recall, F1-Score
    precision = precision_score(ground_truth, labels, average="macro", zero_division=0)
    recall = recall_score(ground_truth, labels, average="macro", zero_division=0)
    f1 = f1_score(ground_truth, labels, average="macro", zero_division=0)
    return purity, accuracy, precision, recall, f1

# Function to match clusters to true labels using the Hungarian algorithm
def match_clusters_to_labels(predicted_labels, true_labels, num_classes):
    # Create a contingency matrix
    contingency_matrix = np.zeros((num_classes, num_classes), dtype=int)
    for i in range(len(true_labels)):
        contingency_matrix[true_labels[i], predicted_labels[i]] += 1

    # Solve the assignment problem using the Hungarian algorithm
    row_ind, col_ind = linear_sum_assignment(-contingency_matrix)
    matched_labels = np.zeros_like(predicted_labels)

    # Assign the matched clusters to the true labels
    for i in range(len(predicted_labels)):
        matched_labels[i] = col_ind[predicted_labels[i]]

    return matched_labels

# Initialize results dictionary
results = {}

# Loop through methods and datasets
for method_name, method in clustering_methods.items():
    for dataset_name, dataset in datasets.items():
        print(f"Processing {dataset_name} with {method_name}...")

        # Extract features and ground truth
        data = dataset.iloc[:, :-1].values  # Exclude ground truth column
        ground_truth = dataset.iloc[:, -1].values  # Assume last column contains ground truth

        # Scale data using StandardScaler
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data)

        # Apply PCA for noise reduction (retain 95% variance)
        pca = PCA(n_components=0.95)
        reduced_data = pca.fit_transform(scaled_data)

        # Apply clustering
        if method_name == "GMM":
            model = method(n_components=cluster_counts[dataset_name], random_state=42)
        else:
            model = method(n_clusters=cluster_counts[dataset_name], random_state=42)

        start_time = time.time()
        predicted_labels = model.fit_predict(reduced_data)
        execution_time = time.time() - start_time

        # Match clusters to true labels (only for GMM or clustering results)
        matched_labels = match_clusters_to_labels(predicted_labels, ground_truth, num_classes=cluster_counts[dataset_name])

        # Calculate metrics
        purity, accuracy, precision, recall, f1 = calculate_metrics(matched_labels, ground_truth)

        # Add to results
        results[f"{dataset_name}-{method_name}"] = {
            "Purity": purity,
            "Accuracy": accuracy,
            "Precision": precision,
            "Recall": recall,
            "F1-Score": f1,
            "Execution Time (s)": execution_time
        }

# Convert results to DataFrame
results_df = pd.DataFrame(results).T
results_df.index.name = "Dataset-Method"
print(results_df)

# Save results to CSV
results_df.to_csv("clustering_metrics_kmeans_kmedoids_gmm_with_label_matching.csv")
print("Results saved to 'clustering_metrics_kmeans_kmedoids_gmm_with_label_matching.csv'")

#applying differet clustering methods on the dataset to evaluate the best
import time
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering, DBSCAN, SpectralClustering, Birch, OPTICS
from scipy.cluster.hierarchy import linkage, fcluster

# clustering methods
clustering_methods = {
    "Agglomerative (Ward)": lambda data, n_clusters: AgglomerativeClustering(n_clusters=n_clusters, linkage='ward').fit_predict(data),
    "Agglomerative (Complete)": lambda data, n_clusters: AgglomerativeClustering(n_clusters=n_clusters, linkage='complete').fit_predict(data),
    "DBSCAN": lambda data, _: DBSCAN(eps=0.5, min_samples=5).fit_predict(data),
    "Spectral Clustering": lambda data, n_clusters: SpectralClustering(n_clusters=n_clusters, random_state=42).fit_predict(data),
    "OPTICS": lambda data, _: OPTICS(min_samples=5, xi=0.05, min_cluster_size=0.1).fit_predict(data),
    "BIRCH": lambda data, n_clusters: Birch(n_clusters=n_clusters).fit_predict(data)
}

# DIANA function
def diana_clustering(data, n_clusters):
    Z = linkage(data, method='ward')  # Hierarchical Ward linkage
    labels = fcluster(Z, t=n_clusters, criterion='maxclust')
    return labels

# Add DIANA to clustering methods
clustering_methods["DIANA"] = lambda data, n_clusters: diana_clustering(data, n_clusters)

# Purity calculation function
def calculate_purity(labels, ground_truth):
    contingency_matrix = pd.crosstab(ground_truth, labels)
    purity = np.sum(np.amax(contingency_matrix.values, axis=0)) / np.sum(contingency_matrix.values)
    return purity

# Load Banknote dataset
banknote_df = pd.read_csv('banknote.csv')

# Preprocess the dataset (PCA to 2D for easier clustering)
data = PCA(n_components=2).fit_transform(banknote_df.iloc[:, :-1])  # Exclude target column

# Number of clusters for Banknote dataset
n_clusters = 2

# Initialize results dictionary
results = {}

# Perform clustering for each method
for method_name, method in clustering_methods.items():
    print(f"Processing Banknote dataset with {method_name}...")

    start_time = time.time()
    try:
        labels = method(data, n_clusters)
        execution_time = time.time() - start_time

        # Calculate metrics if valid clusters are found
        if len(set(labels)) > 1:
            accuracy = accuracy_score(banknote_df.iloc[:, -1], labels)
            precision = precision_score(banknote_df.iloc[:, -1], labels, average="macro", zero_division=0)
            recall = recall_score(banknote_df.iloc[:, -1], labels, average="macro", zero_division=0)
            f1 = f1_score(banknote_df.iloc[:, -1], labels, average="macro", zero_division=0)
            purity = calculate_purity(labels, banknote_df.iloc[:, -1])
        else:
            accuracy = precision = recall = f1 = purity = -1  # Invalid clustering

        # Store results
        results[method_name] = {
            "Accuracy": accuracy,
            "Precision": precision,
            "Recall": recall,
            "F1-Score": f1,
            "Purity": purity,
            "Execution Time (s)": execution_time
        }

    except Exception as e:
        print(f"Error processing Banknote dataset with {method_name}: {e}")
        results[method_name] = {
            "Accuracy": None,
            "Precision": None,
            "Recall": None,
            "F1-Score": None,
            "Purity": None,
            "Execution Time (s)": None
        }

# Convert results to a DataFrame
results_df = pd.DataFrame(results).T
results_df.index.name = "Method"

# Display results
print(results_df)

# Save results to a CSV file
results_df.to_csv("banknote_clustering_metrics_extended.csv")
print("Results saved to 'banknote_clustering_metrics_extended.csv'")



#density function line chart for each 9
import os
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
from scipy.stats import norm

# datasets
datasets = {
    "Banknote": "banknote.csv",
    "Ionosphere": "ionosphere.csv",
    "Sonar": "sonar.csv",
    "Blobs": "blobs.csv",
    "Varied": "varied.csv",
    "Wine": "wine.csv",
    "Flame": "flame.csv",
    "Glass": "glass.csv",
    "Iris": "iris.csv",
}

# number of clusters
cluster_counts = {
    "Banknote": 2,
    "Ionosphere": 2,
    "Sonar": 2,
    "Blobs": 3,
    "Varied": 3,
    "Wine": 3,
    "Flame": 2,
    "Glass": 6,
    "Iris": 3
}

# load datasets
def load_datasets(datasets):
    loaded_datasets = {}
    for name, path in datasets.items():
        if os.path.exists(path):
            loaded_datasets[name] = pd.read_csv(path)
        else:
            print(f"Error: File {path} for dataset {name} not found.")
    return loaded_datasets

# Load datasets
loaded_datasets = load_datasets(datasets)

# Function to plot density functions for GMM
def plot_gmm_density(data, gmm_model, dataset_name):
    means = gmm_model.means_
    covariances = gmm_model.covariances_
    weights = gmm_model.weights_

    # Generate a grid for density plotting
    x = np.linspace(np.min(data[:, 0]), np.max(data[:, 0]), 1000).reshape(-1, 1)
    grid = np.hstack([x, np.zeros_like(x)])  # Ensure consistent dimensionality with 2D data

    plt.figure(figsize=(12, 6))

    # Plot overall density
    overall_density = np.exp(gmm_model.score_samples(grid))
    plt.plot(x[:, 0], overall_density, label='Overall Density', color='black')

    # Plot individual cluster densities
    for mean, cov, weight in zip(means, covariances, weights):
        cluster_density = weight * norm.pdf(x.flatten(), loc=mean[0], scale=np.sqrt(cov[0][0]))
        plt.plot(x.flatten(), cluster_density, label=f'Cluster (mean={mean[0]:.2f})')

    plt.title(f'Density Functions for Each Cluster in {dataset_name}')
    plt.xlabel('Feature Value')
    plt.ylabel('Density')
    plt.legend()
    plt.show()

# Loop through each dataset
for name, dataset in loaded_datasets.items():
    print(f"Processing {name} dataset for GMM density plots...")

    # Preprocess dataset (PCA to reduce to 2D for GMM)
    data = PCA(n_components=2).fit_transform(dataset.iloc[:, :-1])  # Exclude target column

    # Get the number of clusters
    n_clusters = cluster_counts[name]

    # Fit the GMM model
    gmm = GaussianMixture(n_components=n_clusters, random_state=42)
    gmm.fit(data)

    # Generate density plot
    plot_gmm_density(data, gmm, name)

